{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fuzz Up [W.I.P.] fuzzup offers (1) a simple approach for clustering string entitities based on Levenshtein Distance using Fuzzy Matching in conjunction with a simple rule-based clustering method. fuzzup also provides (2) functions for computing the prominence of the resulting entity clusters resulting from (1). fuzzup has been designed to fit the output from NER predictions from the Hugging Face transformers NER pipeline specifically. Installation guide fuzzup can be installed from the Python Package Index (PyPI) by: pip install fuzzup If you want the development version then install directly from Github . Workflow ... COMING SOON! Background fuzzup is developed as a part of Ekstra Bladet \u2019s activities on Platform Intelligence in News (PIN). PIN is an industrial research project that is carried out in collaboration between the Technical University of Denmark , University of Copenhagen and Copenhagen Business School with funding from Innovation Fund Denmark . The project runs from 2020-2023 and develops recommender systems and natural language processing systems geared for news publishing, some of which are open sourced like fuzzup . Read more The detailed documentation and motivation for fuzzup including code references and extended workflow examples can be accessed here . Contact We hope, that you will find fuzzup useful. Please direct any questions and feedbacks to us ! If you want to contribute (which we encourage you to), open a PR . If you encounter a bug or want to suggest an enhancement, please open an issue .","title":"Home"},{"location":"#fuzz-up-wip","text":"fuzzup offers (1) a simple approach for clustering string entitities based on Levenshtein Distance using Fuzzy Matching in conjunction with a simple rule-based clustering method. fuzzup also provides (2) functions for computing the prominence of the resulting entity clusters resulting from (1). fuzzup has been designed to fit the output from NER predictions from the Hugging Face transformers NER pipeline specifically.","title":"Fuzz Up [W.I.P.] "},{"location":"#installation-guide","text":"fuzzup can be installed from the Python Package Index (PyPI) by: pip install fuzzup If you want the development version then install directly from Github .","title":"Installation guide"},{"location":"#workflow","text":"... COMING SOON!","title":"Workflow"},{"location":"#background","text":"fuzzup is developed as a part of Ekstra Bladet \u2019s activities on Platform Intelligence in News (PIN). PIN is an industrial research project that is carried out in collaboration between the Technical University of Denmark , University of Copenhagen and Copenhagen Business School with funding from Innovation Fund Denmark . The project runs from 2020-2023 and develops recommender systems and natural language processing systems geared for news publishing, some of which are open sourced like fuzzup .","title":"Background"},{"location":"#read-more","text":"The detailed documentation and motivation for fuzzup including code references and extended workflow examples can be accessed here .","title":"Read more"},{"location":"#contact","text":"We hope, that you will find fuzzup useful. Please direct any questions and feedbacks to us ! If you want to contribute (which we encourage you to), open a PR . If you encounter a bug or want to suggest an enhancement, please open an issue .","title":"Contact"},{"location":"code_reference/","text":"Code Reference compute_fuzzy_matrix ( strings , ** kwargs ) Compute Fuzzy Matrix Computes matrix with pairwise fuzzy ratios (=edit) distances between all strings. The result can be thought of as a correlation matrix with all diagonal elements equal to 100. Parameters: Name Type Description Default strings List[str] strings for clustering. required kwargs all optional arguments for rapidfuzz.process.cdist. {} Returns: Type Description pd.DataFrame pairwise fuzzy ratios between strings. Examples: >>> person_names = [ 'Donald Trump' , 'Donald Trump' , 'J. biden' , 'joe biden' , 'Biden' , 'Bide' , 'mark esper' , 'Christopher c . miller' , 'jim mattis' , 'Nancy Pelosi' , 'trumps' , 'Trump' , 'Donald' , 'miller' ] .... Source code in fuzzup/fuzz.py def compute_fuzzy_matrix ( strings : List [ str ], ** kwargs ) -> pd . DataFrame : \"\"\"Compute Fuzzy Matrix Computes matrix with pairwise fuzzy ratios (=edit) distances between all strings. The result can be thought of as a correlation matrix with all diagonal elements equal to 100. Args: strings (List[str]): strings for clustering. kwargs: all optional arguments for rapidfuzz.process.cdist. Returns: pd.DataFrame: pairwise fuzzy ratios between strings. Examples: >>> person_names = ['Donald Trump', 'Donald Trump', 'J. biden', 'joe biden', 'Biden', 'Bide', 'mark esper', 'Christopher c . miller', 'jim mattis', 'Nancy Pelosi', 'trumps', 'Trump', 'Donald', 'miller'] .... \"\"\" # subset unique strings. strings = list ( set ( strings )) # compute edit distances dists = cdist ( strings , strings , ** kwargs ) dists = pd . DataFrame ( dists , index = strings , columns = strings ) return dists compute_prominence ( clusters , to_dataframe = False , merge_output = True , weight_position = None , weight_multipliers = None ) Compute Prominence Computes prominence of entity clusters. Parameters: Name Type Description Default clusters List[Dict] Entity clusters. required to_dataframe bool Export output as pandas dataframe? Defaults to False. False merge_output bool Merge resulting cluster meta data with input data. Defaults to True. True weight_position float threshold for position-adjusted weight interpolation. Defaults to None implying no adjustment for positions in text. None weight_multipliers ndarray weight multipliers. None Returns: Type Description List[Dict] clusters and their prominence. Examples: ... Source code in fuzzup/fuzz.py def compute_prominence ( clusters : List [ Dict ], to_dataframe : bool = False , merge_output : bool = True , weight_position : float = None , weight_multipliers : np . ndarray = None ) -> List [ Dict ]: \"\"\"Compute Prominence Computes prominence of entity clusters. Args: clusters (List[Dict]): Entity clusters. to_dataframe (bool, optional): Export output as pandas dataframe? Defaults to False. merge_output (bool, optional): Merge resulting cluster meta data with input data. Defaults to True. weight_position: threshold for position-adjusted weight interpolation. Defaults to None implying no adjustment for positions in text. weight_multipliers: weight multipliers. Returns: List[Dict]: clusters and their prominence. Examples: ... \"\"\" # handle trivial case (empty list) if len ( clusters ) == 0 : if to_dataframe : return pd . DataFrame () else : return [] # validate inputs if weight_position is not None : assert 0 <= weight_position <= 1 , \"choose 'weight_position' between 0 and 1\" if weight_multipliers is not None : assert len ( weight_multipliers ) == len ( clusters ), \"Multipliers must have same length as number of entities\" else : weight_multipliers = float ( 1 ) clusters = pd . DataFrame . from_dict ( clusters ) prominence = clusters . copy () prominence_score = float ( 1 ) # adjust prominence score for word positions (=offsets) if weight_position is not None and len ( clusters . start ) > 1 : offset_min = min ( clusters . start ) offset_max = max ( clusters . start ) # linear interpolation xp = [ offset_min , offset_max ] yp = [ 1 , weight_position ] prominence_position = np . array ([ np . interp ( x , xp , yp ) for x in clusters . start ]) else : prominence_position = float ( 1 ) prominence_score = prominence_score * prominence_position * weight_multipliers # aggregate prominence to group level prominence [ 'prominence_score' ] = prominence_score prominence = prominence . groupby ( CLUSTER_ID )[ 'prominence_score' ] . sum () # rank clusters by prominence ranks = rankdata ( prominence , method = \"max\" ) ranks = max ( ranks ) - ranks + 1 # organize output as data frame prominence = pd . DataFrame ( prominence ) prominence [ 'prominence_rank' ] = ranks prominence . reset_index ( level = 0 , inplace = True ) if merge_output : prominence = pd . merge ( clusters , prominence , how = \"left\" ) if not to_dataframe : prominence = prominence . to_dict ( orient = \"records\" ) return prominence compute_prominence_bygroup ( clusters , return_first_rank = False , ** kwargs ) Compute Prominence by Group Computes prominence by entity group. Simply calls compute_prominence() groupwise. Parameters: Name Type Description Default clusters List[Dict] Entity clusters. required kwargs all optional arguments for compute_prominence(). {} Returns: Type Description List[Dict] entity clusters with prominence scores. Source code in fuzzup/fuzz.py def compute_prominence_bygroup ( clusters : List [ Dict ], return_first_rank : bool = False , ** kwargs ) -> List [ Dict ]: \"\"\"Compute Prominence by Group Computes prominence by entity group. Simply calls compute_prominence() groupwise. Args: clusters (List[Dict]): Entity clusters. kwargs: all optional arguments for compute_prominence(). Returns: List[Dict]: entity clusters with prominence scores. \"\"\" # handle trivial case. if len ( clusters ) == 0 : return [] clusters = pd . DataFrame . from_dict ( clusters ) clusters = clusters . groupby ([ 'entity_group' ]) out = [ compute_prominence ( clusters = clusters . get_group ( group ) . to_dict ( orient = \"records\" ), ** kwargs ) for group in clusters . groups ] out = flatten ( out ) #If you only want the most prominent entities returned, pop all entities that are not the most prominent if return_first_rank : out = [ i for i in out if i [ 'prominence_rank' ] == 1 ] return out compute_prominence_placement ( clusters , placement_col = 'placement' , wgt_body = 1.0 , wgt_lead = 2.0 , wgt_title = 3.0 , bygroup = False , ** kwargs ) Compute Prominence from Article Placement Parameters: Name Type Description Default clusters list NER predictions. required placement_col str Name of column containing article placement of entities. Defaults to \"placement\". 'placement' wgt_body float Weight of entities in body text. Defaults to 1.0. 1.0 wgt_lead float Weight of entities in lead text. Defaults to 2.0. 2.0 wgt_title float Weight of entities in title. Defaults to 3.0. 3.0 bygroup bool use compute_prominence_bygroup() in stead of compute_prominence()? Defaults to True. False kwargs all optional arguments for compute_prominence(bygroup). {} Returns: Type Description list predictions with prominence scores. Source code in fuzzup/fuzz.py def compute_prominence_placement ( clusters : list , placement_col : str = \"placement\" , wgt_body : float = 1.0 , wgt_lead : float = 2.0 , wgt_title : float = 3.0 , bygroup : bool = False , ** kwargs ) -> list : \"\"\"Compute Prominence from Article Placement Args: clusters (list): NER predictions. placement_col (str, optional): Name of column containing article placement of entities. Defaults to \"placement\". wgt_body (float, optional): Weight of entities in body text. Defaults to 1.0. wgt_lead (float, optional): Weight of entities in lead text. Defaults to 2.0. wgt_title (float, optional): Weight of entities in title. Defaults to 3.0. bygroup (bool, optional): use compute_prominence_bygroup() in stead of compute_prominence()? Defaults to True. kwargs: all optional arguments for compute_prominence(bygroup). Returns: list: predictions with prominence scores. \"\"\" if len ( clusters ) == 0 : return [] assert all ([ placement_col in x for x in clusters ]), f 'key { placement_col } must be present in all dicts' weights = { 'body' : wgt_body , 'lead' : wgt_lead , 'title' : wgt_title } multipliers = np . array ([ weights . get ( x . get ( placement_col )) for x in clusters ]) if bygroup : prominence_function = compute_prominence_bygroup else : prominence_function = compute_prominence clusters = prominence_function ( clusters , weight_multipliers = multipliers , ** kwargs ) return clusters fuzzy_cluster ( words , cutoff = 70 , to_dataframe = False , merge_output = True , ** kwargs ) summary Parameters: Name Type Description Default words List[Dict] Words/entities for clustering. required cutoff int Cutoff threshold value for fuzzy ratios when forming clusters. Defaults to 70. 70 to_dataframe bool Output as dataframe? Defaults to True. False merge_output bool Merge output with original input? Defaults to False. True Returns: Type Description List[Dict] Clusters of entities. Source code in fuzzup/fuzz.py def fuzzy_cluster ( words : List [ Dict ], cutoff : int = 70 , to_dataframe : bool = False , merge_output : bool = True , ** kwargs ) -> List [ Dict ]: \"\"\"_summary_ Args: words (List[Dict]): Words/entities for clustering. cutoff (int, optional): Cutoff threshold value for fuzzy ratios when forming clusters. Defaults to 70. to_dataframe (bool, optional): Output as dataframe? Defaults to True. merge_output (bool, optional): Merge output with original input? Defaults to False. Returns: List[Dict]: Clusters of entities. \"\"\" # TODO: implement by_entity_group assert isinstance ( words , list ), \"'words' must be a list\" #Remove existing cluster_id entries in words for word in words : if 'cluster_id' in word : del word [ 'cluster_id' ] # handle trivial case (empty list) if not words : if to_dataframe : return pd . DataFrame () else : return [] if isinstance ( words , list ) and all ([ isinstance ( x , dict ) for x in words ]): output_ner = True strings = [ x . get ( 'word' ) for x in words ] else : output_ner = False strings = words # compute fuzzy ratios fuzzy_matrix = compute_fuzzy_matrix ( strings , ** kwargs ) clusters = naive_cluster ( fuzzy_matrix , cutoff = cutoff ) # generate cluster ids (longest entity variation). cluster_ids = [ max ( cluster , key = len ) for cluster in clusters ] # organize output properly (for compatibility with transformers NER pipeline) output = [] for idx , cluster in enumerate ( clusters ): output . append ( pd . DataFrame . from_dict ({ 'word' : cluster , CLUSTER_ID : cluster_ids [ idx ]})) output = pd . concat ( output , ignore_index = True ) # merge output with original input if output_ner and merge_output : output = pd . merge ( pd . DataFrame . from_dict ( words ), output , how = \"left\" ) if not to_dataframe : output = output . to_dict ( orient = \"records\" ) return output fuzzy_cluster_bygroup ( words , ** kwargs ) Fuzzy Cluster By Group Fuzzy clustering by entity group. Simply calls fuzzy_cluster() groupwise. Parameters: Name Type Description Default words List[Dict] Words/entities. required kwargs all optional arguments for fuzzy_cluster(). {} Returns: Type Description List[Dict] entity clusters. Source code in fuzzup/fuzz.py def fuzzy_cluster_bygroup ( words : List [ Dict ], ** kwargs ) -> List [ Dict ]: \"\"\"Fuzzy Cluster By Group Fuzzy clustering by entity group. Simply calls fuzzy_cluster() groupwise. Args: words (List[Dict]): Words/entities. kwargs: all optional arguments for fuzzy_cluster(). Returns: List[Dict]: entity clusters. \"\"\" # handle trivial case. if len ( words ) == 0 : return [] words = pd . DataFrame . from_dict ( words ) words = words . groupby ([ 'entity_group' ]) out = [ fuzzy_cluster ( words = words . get_group ( group ) . to_dict ( orient = \"records\" ), ** kwargs ) for group in words . groups ] out = flatten ( out ) return out naive_cluster ( fuzzy_matrix , cutoff = 70 , ** kwargs ) Naive Clustering Conducts naive clustering based on matrix with pairwise correlations, fuzzy ratios etc. Parameters: Name Type Description Default fuzzy_matrix pd.DataFrame Matrix with pairwise fuzzy ratios between words. required cutoff float Threshold for naive clustering algorithm with respect to pairwise fuzzy ratios. Defaults to 70. 70 Returns: Type Description list resulting clusters. Source code in fuzzup/fuzz.py def naive_cluster ( fuzzy_matrix : pd . DataFrame , cutoff : float = 70 , ** kwargs ) -> list : \"\"\"Naive Clustering Conducts naive clustering based on matrix with pairwise correlations, fuzzy ratios etc. Args: fuzzy_matrix (pd.DataFrame): Matrix with pairwise fuzzy ratios between words. cutoff (float, optional): Threshold for naive clustering algorithm with respect to pairwise fuzzy ratios. Defaults to 70. Returns: list: resulting clusters. \"\"\" m = fuzzy_matrix clusters = [] while len ( m ) > 0 : var = [ m . index . tolist ()[ 0 ]] cluster , m = helper_clustering ( m , var , cutoff = cutoff ) clusters . append ( cluster ) return clusters simulate_ner_data () Simulate NER data Returns: Type Description List[Dict] Simulated NER data. Source code in fuzzup/datasets.py def simulate_ner_data () -> List [ Dict ]: \"\"\"Simulate NER data Returns: List[Dict]: Simulated NER data. \"\"\" PERSONS = [ 'Donald Trump' , 'Donald Trump' , 'J. biden' , 'joe biden' , 'Biden' , 'Bide' , 'mark esper' , 'Christopher c . miller' , 'jim mattis' , 'Nancy Pelosi' , 'trumps' , 'Trump' , 'Donald' , 'miller' ] # align format with output from Hugging Face `transformers` pipeline n = len ( PERSONS ) PERSONS_NER = pd . DataFrame ( data = PERSONS , columns = [ 'word' ]) PERSONS_NER [ \"entity_group\" ] = \"PER\" PERSONS_NER [ \"score\" ] = np . random . sample ( n ) PERSONS_NER [ \"start\" ] = np . random . randint ( 100 , size = n ) PERSONS_NER [ \"end\" ] = np . random . randint ( 100 , size = n ) placements = [ \"title\" , \"lead\" , \"body\" ] PERSONS_NER [ \"placement\" ] = random . choices ( placements , k = n ) PERSONS_NER = PERSONS_NER . to_dict ( orient = \"records\" ) return PERSONS_NER Cities ( Whitelist ) Danish Cities Whitelist of names of Danish cities initialized from the DAWA API. Source code in fuzzup/whitelists.py class Cities ( Whitelist ): \"\"\"Danish Cities Whitelist of names of Danish cities initialized from the DAWA API. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_eblocal_byer , title = 'city' , entity_group = [ 'LOC' ], ** kwargs ) EBLocalNames ( Whitelist ) EB Local Names Whitelist with Ekstra Bladet Local Names. Source code in fuzzup/whitelists.py class EBLocalNames ( Whitelist ): \"\"\"EB Local Names Whitelist with Ekstra Bladet Local Names. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_eblocal_names , title = 'eblocal_name' , entity_group = [ 'LOC' ], ** kwargs ) Municipalities ( Whitelist ) Danish Cities Whitelist of names of Danish Municipalities initialized from the DAWA API. Source code in fuzzup/whitelists.py class Municipalities ( Whitelist ): \"\"\"Danish Cities Whitelist of names of Danish Municipalities initialized from the DAWA API. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_municipalities , title = 'municipality' , entity_group = [ 'LOC' ], ** kwargs ) Neighborhoods ( Whitelist ) Danish Neighborhoods Whitelist of names of Danish Neighborhoods initialized from the DAWA API. Source code in fuzzup/whitelists.py class Neighborhoods ( Whitelist ): \"\"\"Danish Neighborhoods Whitelist of names of Danish Neighborhoods initialized from the DAWA API. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_eblocal_neighborhoods , title = 'neighborhood' , entity_group = [ 'LOC' ], ** kwargs ) Whitelist Whitelist Whitelist objects containing whitelists and relevant meta data regarding how to apply it. Attributes: Name Type Description entity_group str the entity group of interest for the given whitelist. title str title of the type of entity the whitelist relates to. whitelist dict whitelist with keys to match with. The values contain mappings for the given key. Source code in fuzzup/whitelists.py class Whitelist (): \"\"\"Whitelist Whitelist objects containing whitelists and relevant meta data regarding how to apply it. Attributes: entity_group (str): the entity group of interest for the given whitelist. title (str): title of the type of entity the whitelist relates to. whitelist (dict): whitelist with keys to match with. The values contain mappings for the given key. \"\"\" def __init__ ( self , function_load , title , entity_group , ** kwargs ) -> None : self . entity_group = entity_group self . title = title logger . info ( f \"Loading whitelist: { title } \" ) self . whitelist = function_load ( ** kwargs ) logger . info ( \"Done loading.\" ) def __call__ ( self , words : List [ Dict ], ** kwargs ) -> List [ Dict ]: out = match_whitelist ( words = words , whitelist = self . whitelist , entity_group = self . entity_group , ** kwargs ) # TODO: return mappings return out apply_whitelists ( whitelists , clusters , ** kwargs ) Apply Multiple Whitelists Parameters: Name Type Description Default whitelists List[Whitelist] Whitelists. required clusters List[Dict] Results from fuzzy clustering etc. required kwargs all optional arguments for whitelist matching. {} Returns: Type Description Dict output from whitelist applications. Source code in fuzzup/whitelists.py def apply_whitelists ( whitelists : List [ Whitelist ], clusters : List [ Dict ], ** kwargs ) -> pd . DataFrame : \"\"\"Apply Multiple Whitelists Args: whitelists (List[Whitelist]): Whitelists. clusters (List[Dict]): Results from fuzzy clustering etc. kwargs: all optional arguments for whitelist matching. Returns: Dict: output from whitelist applications. \"\"\" out = { wl . title : wl ( clusters , ** kwargs ) for wl in whitelists } return out format_output ( results , columns = [ 'neighborhood_code' , 'city_code' , 'municipality_code' ], drop_duplicates = True ) Format Output Formats output from whitelist format by extracting only specific columns and converting them to a pandas DataFrame. Parameters: Name Type Description Default results List[Dict] Results from Fuzzy Clustering. required columns List[str] Desired columns to extract. Defaults to ['neighborhood_code', 'city_code', 'municipality_code']. ['neighborhood_code', 'city_code', 'municipality_code'] drop_duplicates bool Drop duplicate matches? Defaults to True. True Returns: Type Description pd.DataFrame Output in desired format. Source code in fuzzup/whitelists.py def format_output ( results : List [ Dict ], columns : List [ str ] = [ 'neighborhood_code' , 'city_code' , 'municipality_code' ], drop_duplicates : bool = True ) -> pd . DataFrame : \"\"\"Format Output Formats output from whitelist format by extracting only specific columns and converting them to a pandas DataFrame. Args: results (List[Dict]): Results from Fuzzy Clustering. columns (List[str], optional): Desired columns to extract. Defaults to ['neighborhood_code', 'city_code', 'municipality_code']. drop_duplicates (bool, optional): Drop duplicate matches? Defaults to True. Returns: pd.DataFrame: Output in desired format. \"\"\" results = [ format_helper ( x = results . get ( x ), columns = columns ) for x in results ] results = pd . concat ( results , ignore_index = True ) if drop_duplicates : results . drop_duplicates ( inplace = True , keep = \"first\" ) return results get_byer () Get all byer in DK Source code in fuzzup/whitelists.py def get_byer (): \"\"\"Get all byer in DK\"\"\" url = 'https://api.dataforsyningen.dk/steder?hovedtype=Bebyggelse&undertype=by' byer = requests . get ( url ) . json () records = [] for by in byer : kommuner = [ kommune [ 'navn' ] for kommune in by [ 'kommuner' ]] records . append ( { 'navn' : by [ 'prim\u00e6rtnavn' ], 'indbyggerantal' : by [ 'egenskaber' ][ 'indbyggerantal' ], 'city_code' : by [ 'id' ], 'municipality' : kommuner , } ) df = pd . DataFrame ( records ) return df get_neighborhoods () Get all neighborhoods in DK Source code in fuzzup/whitelists.py def get_neighborhoods (): \"\"\"Get all neighborhoods in DK\"\"\" url = 'https://api.dataforsyningen.dk/steder?hovedtype=Bebyggelse&undertype=bydel' hoods = requests . get ( url ) . json () out = { hood [ 'prim\u00e6rtnavn' ] : { 'eblocal_code' : hood [ 'id' ]} for hood in hoods } return out get_politicians () copy pasta from https://github.com/cfblaeb/politik Source code in fuzzup/whitelists.py def get_politicians (): \"\"\" copy pasta from https://github.com/cfblaeb/politik \"\"\" # ft.dk only allows 100 rows per call table = 'Akt\u00f8r' url = f 'http://oda.ft.dk/api/ { table } ' totalcount = int ( requests . get ( url , params = { \"$inlinecount\" : \"allpages\" }) . json ()[ 'odata.count' ]) ccount = 0 print ( f \"# records: { totalcount } \" ) results = [] while ccount < totalcount : r = requests . get ( url , params = { \"$skip\" : ccount }) for row in r . json ()[ 'value' ]: if row . get ( 'typeid' ) == 5 : # Type_ID 5 = Politiker i folketinget. if all ([ row . get ( 'slutdato' ) is None , row . get ( 'startdato' ) is not None , row . get ( 'fornavn' ) is not None , row . get ( 'efternavn' ) is not None ]): results . append ( row ) else : pass ccount += 100 if ccount % 1000 == 0 : print ( f \"# records processed: { ccount } / { totalcount } \" ) print ( f \"Number of politicians identified: { len ( results ) } \" ) # extract names names = [ x . get ( 'fornavn' ) + \" \" + x . get ( 'efternavn' ) for x in results ] names = [ clean_string ( x ) . strip () for x in names ] names . sort () # convert to fuzzup dict format names = { x : {} for x in names } return names match_whitelist ( words , whitelist , score_cutoff = 80 , to_dataframe = False , aggregate_cluster = False , entity_group = None , ** kwargs ) Match entities with white list Parameters: Name Type Description Default words List[Dict] words/entities for matching. required whitelist List[str] white list with words/entities to match with. required score_cutoff float Cutoff threshold value for matching. Defaults to 80. 80 to_dataframe bool Return output as data frame. Defaults to False. False aggregate_cluster bool Aggregate matches to cluster level. Defaults to False. False kwargs optinal arguments for cdist. {} entity_group List[str] which entity groups to match. None Returns: Type Description List[Dict] words and their respective matches with the white list. Source code in fuzzup/whitelists.py def match_whitelist ( words : List [ Dict ], whitelist : List [ str ], score_cutoff : float = 80 , to_dataframe : bool = False , aggregate_cluster : bool = False , entity_group : List [ str ] = None , ** kwargs ) -> List [ Dict ]: \"\"\"Match entities with white list Args: words (List[Dict]): words/entities for matching. whitelist (List[str]): white list with words/entities to match with. score_cutoff (float, optional): Cutoff threshold value for matching. Defaults to 80. to_dataframe (bool, optional): Return output as data frame. Defaults to False. aggregate_cluster (bool, optional): Aggregate matches to cluster level. Defaults to False. kwargs: optinal arguments for cdist. entity_group: which entity groups to match. Returns: List[Dict]: words and their respective matches with the white list. \"\"\" assert isinstance ( words , list ), \"'words' must be a list\" assert isinstance ( whitelist , ( list , dict )), \"'whitelist' must be a list or dit\" is_dict = False if isinstance ( whitelist , dict ): is_dict = True whitelist_dict = whitelist whitelist = list ( whitelist . keys ()) # handle trivial case (empty list) if not words or not whitelist : if to_dataframe : return pd . DataFrame () else : return [] if isinstance ( words , list ) and all ([ isinstance ( x , dict ) for x in words ]): output_ner = True if entity_group is not None : words = [ x for x in words if x . get ( 'entity_group' ) in entity_group ] strings = [ x . get ( 'word' ) for x in words ] else : output_ner = False strings = words if len ( strings ) == 0 : if to_dataframe : return pd . DataFrame () else : return [] # compute distances dists = cdist ( whitelist , strings , score_cutoff = score_cutoff , ** kwargs ) matches = [ np . array ( whitelist )[ np . where ( col )] for col in dists . T ] if not output_ner : df = pd . DataFrame . from_dict ({ 'word' : strings , 'matches' : matches }) if output_ner : df = pd . DataFrame . from_records ( words ) df [ \"matches\" ] = matches if aggregate_cluster : matches = pd . DataFrame ( df . groupby ( by = [ 'cluster_id' ]) . apply ( aggregate_to_cluster ), columns = [ 'matches' ], index = None ) matches = matches . reset_index () df . drop ( 'matches' , axis = 1 , inplace = True ) df = pd . merge ( df , matches , how = \"left\" ) df [ 'matches' ] = [ x . tolist () for x in df [ 'matches' ]] if is_dict : mappings = [] for match in df . matches . tolist (): out = [ whitelist_dict . get ( x ) for x in match ] mappings . append ( out ) df [ 'mappings' ] = mappings # subset matches only df = df [ df [ 'matches' ] . astype ( str ) != '[]' ] if not to_dataframe : df = df . to_dict ( orient = \"records\" ) return df","title":"Code Reference"},{"location":"code_reference/#code-reference","text":"","title":"Code Reference"},{"location":"code_reference/#fuzzup.fuzz.compute_fuzzy_matrix","text":"Compute Fuzzy Matrix Computes matrix with pairwise fuzzy ratios (=edit) distances between all strings. The result can be thought of as a correlation matrix with all diagonal elements equal to 100. Parameters: Name Type Description Default strings List[str] strings for clustering. required kwargs all optional arguments for rapidfuzz.process.cdist. {} Returns: Type Description pd.DataFrame pairwise fuzzy ratios between strings. Examples: >>> person_names = [ 'Donald Trump' , 'Donald Trump' , 'J. biden' , 'joe biden' , 'Biden' , 'Bide' , 'mark esper' , 'Christopher c . miller' , 'jim mattis' , 'Nancy Pelosi' , 'trumps' , 'Trump' , 'Donald' , 'miller' ] .... Source code in fuzzup/fuzz.py def compute_fuzzy_matrix ( strings : List [ str ], ** kwargs ) -> pd . DataFrame : \"\"\"Compute Fuzzy Matrix Computes matrix with pairwise fuzzy ratios (=edit) distances between all strings. The result can be thought of as a correlation matrix with all diagonal elements equal to 100. Args: strings (List[str]): strings for clustering. kwargs: all optional arguments for rapidfuzz.process.cdist. Returns: pd.DataFrame: pairwise fuzzy ratios between strings. Examples: >>> person_names = ['Donald Trump', 'Donald Trump', 'J. biden', 'joe biden', 'Biden', 'Bide', 'mark esper', 'Christopher c . miller', 'jim mattis', 'Nancy Pelosi', 'trumps', 'Trump', 'Donald', 'miller'] .... \"\"\" # subset unique strings. strings = list ( set ( strings )) # compute edit distances dists = cdist ( strings , strings , ** kwargs ) dists = pd . DataFrame ( dists , index = strings , columns = strings ) return dists","title":"compute_fuzzy_matrix()"},{"location":"code_reference/#fuzzup.fuzz.compute_prominence","text":"Compute Prominence Computes prominence of entity clusters. Parameters: Name Type Description Default clusters List[Dict] Entity clusters. required to_dataframe bool Export output as pandas dataframe? Defaults to False. False merge_output bool Merge resulting cluster meta data with input data. Defaults to True. True weight_position float threshold for position-adjusted weight interpolation. Defaults to None implying no adjustment for positions in text. None weight_multipliers ndarray weight multipliers. None Returns: Type Description List[Dict] clusters and their prominence. Examples: ... Source code in fuzzup/fuzz.py def compute_prominence ( clusters : List [ Dict ], to_dataframe : bool = False , merge_output : bool = True , weight_position : float = None , weight_multipliers : np . ndarray = None ) -> List [ Dict ]: \"\"\"Compute Prominence Computes prominence of entity clusters. Args: clusters (List[Dict]): Entity clusters. to_dataframe (bool, optional): Export output as pandas dataframe? Defaults to False. merge_output (bool, optional): Merge resulting cluster meta data with input data. Defaults to True. weight_position: threshold for position-adjusted weight interpolation. Defaults to None implying no adjustment for positions in text. weight_multipliers: weight multipliers. Returns: List[Dict]: clusters and their prominence. Examples: ... \"\"\" # handle trivial case (empty list) if len ( clusters ) == 0 : if to_dataframe : return pd . DataFrame () else : return [] # validate inputs if weight_position is not None : assert 0 <= weight_position <= 1 , \"choose 'weight_position' between 0 and 1\" if weight_multipliers is not None : assert len ( weight_multipliers ) == len ( clusters ), \"Multipliers must have same length as number of entities\" else : weight_multipliers = float ( 1 ) clusters = pd . DataFrame . from_dict ( clusters ) prominence = clusters . copy () prominence_score = float ( 1 ) # adjust prominence score for word positions (=offsets) if weight_position is not None and len ( clusters . start ) > 1 : offset_min = min ( clusters . start ) offset_max = max ( clusters . start ) # linear interpolation xp = [ offset_min , offset_max ] yp = [ 1 , weight_position ] prominence_position = np . array ([ np . interp ( x , xp , yp ) for x in clusters . start ]) else : prominence_position = float ( 1 ) prominence_score = prominence_score * prominence_position * weight_multipliers # aggregate prominence to group level prominence [ 'prominence_score' ] = prominence_score prominence = prominence . groupby ( CLUSTER_ID )[ 'prominence_score' ] . sum () # rank clusters by prominence ranks = rankdata ( prominence , method = \"max\" ) ranks = max ( ranks ) - ranks + 1 # organize output as data frame prominence = pd . DataFrame ( prominence ) prominence [ 'prominence_rank' ] = ranks prominence . reset_index ( level = 0 , inplace = True ) if merge_output : prominence = pd . merge ( clusters , prominence , how = \"left\" ) if not to_dataframe : prominence = prominence . to_dict ( orient = \"records\" ) return prominence","title":"compute_prominence()"},{"location":"code_reference/#fuzzup.fuzz.compute_prominence_bygroup","text":"Compute Prominence by Group Computes prominence by entity group. Simply calls compute_prominence() groupwise. Parameters: Name Type Description Default clusters List[Dict] Entity clusters. required kwargs all optional arguments for compute_prominence(). {} Returns: Type Description List[Dict] entity clusters with prominence scores. Source code in fuzzup/fuzz.py def compute_prominence_bygroup ( clusters : List [ Dict ], return_first_rank : bool = False , ** kwargs ) -> List [ Dict ]: \"\"\"Compute Prominence by Group Computes prominence by entity group. Simply calls compute_prominence() groupwise. Args: clusters (List[Dict]): Entity clusters. kwargs: all optional arguments for compute_prominence(). Returns: List[Dict]: entity clusters with prominence scores. \"\"\" # handle trivial case. if len ( clusters ) == 0 : return [] clusters = pd . DataFrame . from_dict ( clusters ) clusters = clusters . groupby ([ 'entity_group' ]) out = [ compute_prominence ( clusters = clusters . get_group ( group ) . to_dict ( orient = \"records\" ), ** kwargs ) for group in clusters . groups ] out = flatten ( out ) #If you only want the most prominent entities returned, pop all entities that are not the most prominent if return_first_rank : out = [ i for i in out if i [ 'prominence_rank' ] == 1 ] return out","title":"compute_prominence_bygroup()"},{"location":"code_reference/#fuzzup.fuzz.compute_prominence_placement","text":"Compute Prominence from Article Placement Parameters: Name Type Description Default clusters list NER predictions. required placement_col str Name of column containing article placement of entities. Defaults to \"placement\". 'placement' wgt_body float Weight of entities in body text. Defaults to 1.0. 1.0 wgt_lead float Weight of entities in lead text. Defaults to 2.0. 2.0 wgt_title float Weight of entities in title. Defaults to 3.0. 3.0 bygroup bool use compute_prominence_bygroup() in stead of compute_prominence()? Defaults to True. False kwargs all optional arguments for compute_prominence(bygroup). {} Returns: Type Description list predictions with prominence scores. Source code in fuzzup/fuzz.py def compute_prominence_placement ( clusters : list , placement_col : str = \"placement\" , wgt_body : float = 1.0 , wgt_lead : float = 2.0 , wgt_title : float = 3.0 , bygroup : bool = False , ** kwargs ) -> list : \"\"\"Compute Prominence from Article Placement Args: clusters (list): NER predictions. placement_col (str, optional): Name of column containing article placement of entities. Defaults to \"placement\". wgt_body (float, optional): Weight of entities in body text. Defaults to 1.0. wgt_lead (float, optional): Weight of entities in lead text. Defaults to 2.0. wgt_title (float, optional): Weight of entities in title. Defaults to 3.0. bygroup (bool, optional): use compute_prominence_bygroup() in stead of compute_prominence()? Defaults to True. kwargs: all optional arguments for compute_prominence(bygroup). Returns: list: predictions with prominence scores. \"\"\" if len ( clusters ) == 0 : return [] assert all ([ placement_col in x for x in clusters ]), f 'key { placement_col } must be present in all dicts' weights = { 'body' : wgt_body , 'lead' : wgt_lead , 'title' : wgt_title } multipliers = np . array ([ weights . get ( x . get ( placement_col )) for x in clusters ]) if bygroup : prominence_function = compute_prominence_bygroup else : prominence_function = compute_prominence clusters = prominence_function ( clusters , weight_multipliers = multipliers , ** kwargs ) return clusters","title":"compute_prominence_placement()"},{"location":"code_reference/#fuzzup.fuzz.fuzzy_cluster","text":"summary Parameters: Name Type Description Default words List[Dict] Words/entities for clustering. required cutoff int Cutoff threshold value for fuzzy ratios when forming clusters. Defaults to 70. 70 to_dataframe bool Output as dataframe? Defaults to True. False merge_output bool Merge output with original input? Defaults to False. True Returns: Type Description List[Dict] Clusters of entities. Source code in fuzzup/fuzz.py def fuzzy_cluster ( words : List [ Dict ], cutoff : int = 70 , to_dataframe : bool = False , merge_output : bool = True , ** kwargs ) -> List [ Dict ]: \"\"\"_summary_ Args: words (List[Dict]): Words/entities for clustering. cutoff (int, optional): Cutoff threshold value for fuzzy ratios when forming clusters. Defaults to 70. to_dataframe (bool, optional): Output as dataframe? Defaults to True. merge_output (bool, optional): Merge output with original input? Defaults to False. Returns: List[Dict]: Clusters of entities. \"\"\" # TODO: implement by_entity_group assert isinstance ( words , list ), \"'words' must be a list\" #Remove existing cluster_id entries in words for word in words : if 'cluster_id' in word : del word [ 'cluster_id' ] # handle trivial case (empty list) if not words : if to_dataframe : return pd . DataFrame () else : return [] if isinstance ( words , list ) and all ([ isinstance ( x , dict ) for x in words ]): output_ner = True strings = [ x . get ( 'word' ) for x in words ] else : output_ner = False strings = words # compute fuzzy ratios fuzzy_matrix = compute_fuzzy_matrix ( strings , ** kwargs ) clusters = naive_cluster ( fuzzy_matrix , cutoff = cutoff ) # generate cluster ids (longest entity variation). cluster_ids = [ max ( cluster , key = len ) for cluster in clusters ] # organize output properly (for compatibility with transformers NER pipeline) output = [] for idx , cluster in enumerate ( clusters ): output . append ( pd . DataFrame . from_dict ({ 'word' : cluster , CLUSTER_ID : cluster_ids [ idx ]})) output = pd . concat ( output , ignore_index = True ) # merge output with original input if output_ner and merge_output : output = pd . merge ( pd . DataFrame . from_dict ( words ), output , how = \"left\" ) if not to_dataframe : output = output . to_dict ( orient = \"records\" ) return output","title":"fuzzy_cluster()"},{"location":"code_reference/#fuzzup.fuzz.fuzzy_cluster_bygroup","text":"Fuzzy Cluster By Group Fuzzy clustering by entity group. Simply calls fuzzy_cluster() groupwise. Parameters: Name Type Description Default words List[Dict] Words/entities. required kwargs all optional arguments for fuzzy_cluster(). {} Returns: Type Description List[Dict] entity clusters. Source code in fuzzup/fuzz.py def fuzzy_cluster_bygroup ( words : List [ Dict ], ** kwargs ) -> List [ Dict ]: \"\"\"Fuzzy Cluster By Group Fuzzy clustering by entity group. Simply calls fuzzy_cluster() groupwise. Args: words (List[Dict]): Words/entities. kwargs: all optional arguments for fuzzy_cluster(). Returns: List[Dict]: entity clusters. \"\"\" # handle trivial case. if len ( words ) == 0 : return [] words = pd . DataFrame . from_dict ( words ) words = words . groupby ([ 'entity_group' ]) out = [ fuzzy_cluster ( words = words . get_group ( group ) . to_dict ( orient = \"records\" ), ** kwargs ) for group in words . groups ] out = flatten ( out ) return out","title":"fuzzy_cluster_bygroup()"},{"location":"code_reference/#fuzzup.fuzz.naive_cluster","text":"Naive Clustering Conducts naive clustering based on matrix with pairwise correlations, fuzzy ratios etc. Parameters: Name Type Description Default fuzzy_matrix pd.DataFrame Matrix with pairwise fuzzy ratios between words. required cutoff float Threshold for naive clustering algorithm with respect to pairwise fuzzy ratios. Defaults to 70. 70 Returns: Type Description list resulting clusters. Source code in fuzzup/fuzz.py def naive_cluster ( fuzzy_matrix : pd . DataFrame , cutoff : float = 70 , ** kwargs ) -> list : \"\"\"Naive Clustering Conducts naive clustering based on matrix with pairwise correlations, fuzzy ratios etc. Args: fuzzy_matrix (pd.DataFrame): Matrix with pairwise fuzzy ratios between words. cutoff (float, optional): Threshold for naive clustering algorithm with respect to pairwise fuzzy ratios. Defaults to 70. Returns: list: resulting clusters. \"\"\" m = fuzzy_matrix clusters = [] while len ( m ) > 0 : var = [ m . index . tolist ()[ 0 ]] cluster , m = helper_clustering ( m , var , cutoff = cutoff ) clusters . append ( cluster ) return clusters","title":"naive_cluster()"},{"location":"code_reference/#fuzzup.datasets.simulate_ner_data","text":"Simulate NER data Returns: Type Description List[Dict] Simulated NER data. Source code in fuzzup/datasets.py def simulate_ner_data () -> List [ Dict ]: \"\"\"Simulate NER data Returns: List[Dict]: Simulated NER data. \"\"\" PERSONS = [ 'Donald Trump' , 'Donald Trump' , 'J. biden' , 'joe biden' , 'Biden' , 'Bide' , 'mark esper' , 'Christopher c . miller' , 'jim mattis' , 'Nancy Pelosi' , 'trumps' , 'Trump' , 'Donald' , 'miller' ] # align format with output from Hugging Face `transformers` pipeline n = len ( PERSONS ) PERSONS_NER = pd . DataFrame ( data = PERSONS , columns = [ 'word' ]) PERSONS_NER [ \"entity_group\" ] = \"PER\" PERSONS_NER [ \"score\" ] = np . random . sample ( n ) PERSONS_NER [ \"start\" ] = np . random . randint ( 100 , size = n ) PERSONS_NER [ \"end\" ] = np . random . randint ( 100 , size = n ) placements = [ \"title\" , \"lead\" , \"body\" ] PERSONS_NER [ \"placement\" ] = random . choices ( placements , k = n ) PERSONS_NER = PERSONS_NER . to_dict ( orient = \"records\" ) return PERSONS_NER","title":"simulate_ner_data()"},{"location":"code_reference/#fuzzup.whitelists.Cities","text":"Danish Cities Whitelist of names of Danish cities initialized from the DAWA API. Source code in fuzzup/whitelists.py class Cities ( Whitelist ): \"\"\"Danish Cities Whitelist of names of Danish cities initialized from the DAWA API. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_eblocal_byer , title = 'city' , entity_group = [ 'LOC' ], ** kwargs )","title":"Cities"},{"location":"code_reference/#fuzzup.whitelists.EBLocalNames","text":"EB Local Names Whitelist with Ekstra Bladet Local Names. Source code in fuzzup/whitelists.py class EBLocalNames ( Whitelist ): \"\"\"EB Local Names Whitelist with Ekstra Bladet Local Names. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_eblocal_names , title = 'eblocal_name' , entity_group = [ 'LOC' ], ** kwargs )","title":"EBLocalNames"},{"location":"code_reference/#fuzzup.whitelists.Municipalities","text":"Danish Cities Whitelist of names of Danish Municipalities initialized from the DAWA API. Source code in fuzzup/whitelists.py class Municipalities ( Whitelist ): \"\"\"Danish Cities Whitelist of names of Danish Municipalities initialized from the DAWA API. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_municipalities , title = 'municipality' , entity_group = [ 'LOC' ], ** kwargs )","title":"Municipalities"},{"location":"code_reference/#fuzzup.whitelists.Neighborhoods","text":"Danish Neighborhoods Whitelist of names of Danish Neighborhoods initialized from the DAWA API. Source code in fuzzup/whitelists.py class Neighborhoods ( Whitelist ): \"\"\"Danish Neighborhoods Whitelist of names of Danish Neighborhoods initialized from the DAWA API. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( function_load = get_eblocal_neighborhoods , title = 'neighborhood' , entity_group = [ 'LOC' ], ** kwargs )","title":"Neighborhoods"},{"location":"code_reference/#fuzzup.whitelists.Whitelist","text":"Whitelist Whitelist objects containing whitelists and relevant meta data regarding how to apply it. Attributes: Name Type Description entity_group str the entity group of interest for the given whitelist. title str title of the type of entity the whitelist relates to. whitelist dict whitelist with keys to match with. The values contain mappings for the given key. Source code in fuzzup/whitelists.py class Whitelist (): \"\"\"Whitelist Whitelist objects containing whitelists and relevant meta data regarding how to apply it. Attributes: entity_group (str): the entity group of interest for the given whitelist. title (str): title of the type of entity the whitelist relates to. whitelist (dict): whitelist with keys to match with. The values contain mappings for the given key. \"\"\" def __init__ ( self , function_load , title , entity_group , ** kwargs ) -> None : self . entity_group = entity_group self . title = title logger . info ( f \"Loading whitelist: { title } \" ) self . whitelist = function_load ( ** kwargs ) logger . info ( \"Done loading.\" ) def __call__ ( self , words : List [ Dict ], ** kwargs ) -> List [ Dict ]: out = match_whitelist ( words = words , whitelist = self . whitelist , entity_group = self . entity_group , ** kwargs ) # TODO: return mappings return out","title":"Whitelist"},{"location":"code_reference/#fuzzup.whitelists.apply_whitelists","text":"Apply Multiple Whitelists Parameters: Name Type Description Default whitelists List[Whitelist] Whitelists. required clusters List[Dict] Results from fuzzy clustering etc. required kwargs all optional arguments for whitelist matching. {} Returns: Type Description Dict output from whitelist applications. Source code in fuzzup/whitelists.py def apply_whitelists ( whitelists : List [ Whitelist ], clusters : List [ Dict ], ** kwargs ) -> pd . DataFrame : \"\"\"Apply Multiple Whitelists Args: whitelists (List[Whitelist]): Whitelists. clusters (List[Dict]): Results from fuzzy clustering etc. kwargs: all optional arguments for whitelist matching. Returns: Dict: output from whitelist applications. \"\"\" out = { wl . title : wl ( clusters , ** kwargs ) for wl in whitelists } return out","title":"apply_whitelists()"},{"location":"code_reference/#fuzzup.whitelists.format_output","text":"Format Output Formats output from whitelist format by extracting only specific columns and converting them to a pandas DataFrame. Parameters: Name Type Description Default results List[Dict] Results from Fuzzy Clustering. required columns List[str] Desired columns to extract. Defaults to ['neighborhood_code', 'city_code', 'municipality_code']. ['neighborhood_code', 'city_code', 'municipality_code'] drop_duplicates bool Drop duplicate matches? Defaults to True. True Returns: Type Description pd.DataFrame Output in desired format. Source code in fuzzup/whitelists.py def format_output ( results : List [ Dict ], columns : List [ str ] = [ 'neighborhood_code' , 'city_code' , 'municipality_code' ], drop_duplicates : bool = True ) -> pd . DataFrame : \"\"\"Format Output Formats output from whitelist format by extracting only specific columns and converting them to a pandas DataFrame. Args: results (List[Dict]): Results from Fuzzy Clustering. columns (List[str], optional): Desired columns to extract. Defaults to ['neighborhood_code', 'city_code', 'municipality_code']. drop_duplicates (bool, optional): Drop duplicate matches? Defaults to True. Returns: pd.DataFrame: Output in desired format. \"\"\" results = [ format_helper ( x = results . get ( x ), columns = columns ) for x in results ] results = pd . concat ( results , ignore_index = True ) if drop_duplicates : results . drop_duplicates ( inplace = True , keep = \"first\" ) return results","title":"format_output()"},{"location":"code_reference/#fuzzup.whitelists.get_byer","text":"Get all byer in DK Source code in fuzzup/whitelists.py def get_byer (): \"\"\"Get all byer in DK\"\"\" url = 'https://api.dataforsyningen.dk/steder?hovedtype=Bebyggelse&undertype=by' byer = requests . get ( url ) . json () records = [] for by in byer : kommuner = [ kommune [ 'navn' ] for kommune in by [ 'kommuner' ]] records . append ( { 'navn' : by [ 'prim\u00e6rtnavn' ], 'indbyggerantal' : by [ 'egenskaber' ][ 'indbyggerantal' ], 'city_code' : by [ 'id' ], 'municipality' : kommuner , } ) df = pd . DataFrame ( records ) return df","title":"get_byer()"},{"location":"code_reference/#fuzzup.whitelists.get_neighborhoods","text":"Get all neighborhoods in DK Source code in fuzzup/whitelists.py def get_neighborhoods (): \"\"\"Get all neighborhoods in DK\"\"\" url = 'https://api.dataforsyningen.dk/steder?hovedtype=Bebyggelse&undertype=bydel' hoods = requests . get ( url ) . json () out = { hood [ 'prim\u00e6rtnavn' ] : { 'eblocal_code' : hood [ 'id' ]} for hood in hoods } return out","title":"get_neighborhoods()"},{"location":"code_reference/#fuzzup.whitelists.get_politicians","text":"copy pasta from https://github.com/cfblaeb/politik Source code in fuzzup/whitelists.py def get_politicians (): \"\"\" copy pasta from https://github.com/cfblaeb/politik \"\"\" # ft.dk only allows 100 rows per call table = 'Akt\u00f8r' url = f 'http://oda.ft.dk/api/ { table } ' totalcount = int ( requests . get ( url , params = { \"$inlinecount\" : \"allpages\" }) . json ()[ 'odata.count' ]) ccount = 0 print ( f \"# records: { totalcount } \" ) results = [] while ccount < totalcount : r = requests . get ( url , params = { \"$skip\" : ccount }) for row in r . json ()[ 'value' ]: if row . get ( 'typeid' ) == 5 : # Type_ID 5 = Politiker i folketinget. if all ([ row . get ( 'slutdato' ) is None , row . get ( 'startdato' ) is not None , row . get ( 'fornavn' ) is not None , row . get ( 'efternavn' ) is not None ]): results . append ( row ) else : pass ccount += 100 if ccount % 1000 == 0 : print ( f \"# records processed: { ccount } / { totalcount } \" ) print ( f \"Number of politicians identified: { len ( results ) } \" ) # extract names names = [ x . get ( 'fornavn' ) + \" \" + x . get ( 'efternavn' ) for x in results ] names = [ clean_string ( x ) . strip () for x in names ] names . sort () # convert to fuzzup dict format names = { x : {} for x in names } return names","title":"get_politicians()"},{"location":"code_reference/#fuzzup.whitelists.match_whitelist","text":"Match entities with white list Parameters: Name Type Description Default words List[Dict] words/entities for matching. required whitelist List[str] white list with words/entities to match with. required score_cutoff float Cutoff threshold value for matching. Defaults to 80. 80 to_dataframe bool Return output as data frame. Defaults to False. False aggregate_cluster bool Aggregate matches to cluster level. Defaults to False. False kwargs optinal arguments for cdist. {} entity_group List[str] which entity groups to match. None Returns: Type Description List[Dict] words and their respective matches with the white list. Source code in fuzzup/whitelists.py def match_whitelist ( words : List [ Dict ], whitelist : List [ str ], score_cutoff : float = 80 , to_dataframe : bool = False , aggregate_cluster : bool = False , entity_group : List [ str ] = None , ** kwargs ) -> List [ Dict ]: \"\"\"Match entities with white list Args: words (List[Dict]): words/entities for matching. whitelist (List[str]): white list with words/entities to match with. score_cutoff (float, optional): Cutoff threshold value for matching. Defaults to 80. to_dataframe (bool, optional): Return output as data frame. Defaults to False. aggregate_cluster (bool, optional): Aggregate matches to cluster level. Defaults to False. kwargs: optinal arguments for cdist. entity_group: which entity groups to match. Returns: List[Dict]: words and their respective matches with the white list. \"\"\" assert isinstance ( words , list ), \"'words' must be a list\" assert isinstance ( whitelist , ( list , dict )), \"'whitelist' must be a list or dit\" is_dict = False if isinstance ( whitelist , dict ): is_dict = True whitelist_dict = whitelist whitelist = list ( whitelist . keys ()) # handle trivial case (empty list) if not words or not whitelist : if to_dataframe : return pd . DataFrame () else : return [] if isinstance ( words , list ) and all ([ isinstance ( x , dict ) for x in words ]): output_ner = True if entity_group is not None : words = [ x for x in words if x . get ( 'entity_group' ) in entity_group ] strings = [ x . get ( 'word' ) for x in words ] else : output_ner = False strings = words if len ( strings ) == 0 : if to_dataframe : return pd . DataFrame () else : return [] # compute distances dists = cdist ( whitelist , strings , score_cutoff = score_cutoff , ** kwargs ) matches = [ np . array ( whitelist )[ np . where ( col )] for col in dists . T ] if not output_ner : df = pd . DataFrame . from_dict ({ 'word' : strings , 'matches' : matches }) if output_ner : df = pd . DataFrame . from_records ( words ) df [ \"matches\" ] = matches if aggregate_cluster : matches = pd . DataFrame ( df . groupby ( by = [ 'cluster_id' ]) . apply ( aggregate_to_cluster ), columns = [ 'matches' ], index = None ) matches = matches . reset_index () df . drop ( 'matches' , axis = 1 , inplace = True ) df = pd . merge ( df , matches , how = \"left\" ) df [ 'matches' ] = [ x . tolist () for x in df [ 'matches' ]] if is_dict : mappings = [] for match in df . matches . tolist (): out = [ whitelist_dict . get ( x ) for x in match ] mappings . append ( out ) df [ 'mappings' ] = mappings # subset matches only df = df [ df [ 'matches' ] . astype ( str ) != '[]' ] if not to_dataframe : df = df . to_dict ( orient = \"records\" ) return df","title":"match_whitelist()"},{"location":"showcase/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); fuzzup Showcase fuzzup offers (1) a simple approach for clustering string entitities based on Levenshtein Distance using Fuzzy Matching in conjunction with a simple rule-based clustering method. fuzzup also provides (2) functions for computing the prominence of entity clusters resulting from (1). In this section we will go through the nuts and bolts of fuzzup by applying it to a realistic setting. Designed for Handling Output from NER An important use-case for fuzzup is organizing, structuring and analyzing output from Named-Entity Recognition (=NER). For this reason fuzzup has been handtailored to fit the output from NER predictions from the Hugging Face transformers NER pipeline specifically. Use-case First of, import dependencies needed later. from rapidfuzz.fuzz import partial_token_set_ratio import pandas as pd import numpy as np from fuzzup.datasets import simulate_ner_data from fuzzup.fuzz import ( fuzzy_cluster, compute_prominence, compute_fuzzy_matrix, ) from fuzzup.whitelists import match_whitelist Say, we have used a transformers Hugging Face NER pipeline to identify names of persons in a news article. The output from the algorithm is a list of string entities and looks like this (simulated data). PERSONS_NER = simulate_ner_data() pd.DataFrame.from_records(PERSONS_NER) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement 0 Donald Trump PER 0.778987 47 35 body 1 Donald Trump PER 0.466251 55 40 lead 2 J. biden PER 0.651698 77 21 body 3 joe biden PER 0.808048 31 72 body 4 Biden PER 0.899257 84 37 title 5 Bide PER 0.663719 93 61 body 6 mark esper PER 0.928882 32 68 title 7 Christopher c . miller PER 0.990222 94 6 title 8 jim mattis PER 0.747955 11 31 title 9 Nancy Pelosi PER 0.201458 31 92 body 10 trumps PER 0.977139 75 43 body 11 Trump PER 0.526853 81 38 lead 12 Donald PER 0.480917 34 32 body 13 miller PER 0.131910 59 57 body As you can see, the output is rather messy (partly due to the stochastic nature of the algorithm). Another reason for the output looking messy is, that for instance 'Joe Biden' has been mentioned a lot of times but in different ways, e.g. 'Joe Biden', 'J. Biden' and 'Biden'. We want to organize these strings entities by forming meaningful clusters from them, in which the entities are closely related based on their pairwise edit distances. Workflow fuzzup offers functionality for: Computing all of the mutual string distances (Levensteihn Distances/fuzzy ratios) between the string entities Forming clusters of string entities based on the distances from (1) Computing prominence of the clusters from (2) based on the number of entity occurrences, their positions in the text etc. Matching entities (clusters) with entity whitelists Together these steps constitute an end-to-end approach for organizing and structuring the output from NER. Below we go through a simple example of the fuzzup workflow. Step 1: Compute Pairwise Edit Distances First, fuzzup computes pairwise fuzzy ratios for all pairs of string entities. Fuzzy ratios are numbers between 0 and 100 are measures of similarity between strings. They are derived from the Levenshtein distance - a string metric, that measures the distance between two strings. In short the Levenshtein distance (also known as 'edit distance') between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. fuzzup has a separate function compute_fuzzy_matrix for this, that presents the output - the mutual fuzzy ratios - as a cross-tabular matrix with all ratios. from fuzzup.fuzz import fuzzy_cluster persons = [x.get('word') for x in PERSONS_NER] compute_fuzzy_matrix(persons, scorer=partial_token_set_ratio) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Trump jim mattis Bide joe biden Donald Biden trumps Donald Trump mark esper miller J. biden Christopher c . miller Nancy Pelosi Trump 100.000000 25.000000 0.000000 0.000000 0.000000 0.000000 80.000000 100.000000 40.000000 33.333332 0.000000 40.000000 0.000000 jim mattis 25.000000 100.000000 33.333332 30.769230 18.181818 28.571428 44.444443 25.000000 40.000000 33.333332 26.666666 35.294117 23.529411 Bide 0.000000 33.333332 100.000000 75.000000 40.000000 100.000000 0.000000 25.000000 40.000000 50.000000 75.000000 50.000000 40.000000 joe biden 0.000000 30.769230 75.000000 100.000000 25.000000 80.000000 0.000000 25.000000 30.769230 40.000000 100.000000 33.333332 35.294117 Donald 0.000000 18.181818 40.000000 25.000000 100.000000 33.333332 0.000000 100.000000 22.222221 22.222221 28.571428 22.222221 25.000000 Biden 0.000000 28.571428 100.000000 80.000000 33.333332 100.000000 0.000000 25.000000 33.333332 40.000000 88.888885 40.000000 33.333332 trumps 80.000000 44.444443 0.000000 0.000000 0.000000 0.000000 100.000000 80.000000 33.333332 28.571428 0.000000 33.333332 25.000000 Donald Trump 100.000000 25.000000 25.000000 25.000000 100.000000 25.000000 80.000000 100.000000 28.571428 33.333332 18.181818 27.272728 22.222221 mark esper 40.000000 37.500000 40.000000 30.769230 22.222221 33.333332 33.333332 28.571428 100.000000 40.000000 22.222221 50.000000 26.666666 miller 33.333332 33.333332 50.000000 40.000000 25.000000 40.000000 25.000000 33.333332 40.000000 100.000000 40.000000 100.000000 28.571428 J. biden 0.000000 26.666666 75.000000 100.000000 28.571428 88.888885 0.000000 18.181818 22.222221 40.000000 100.000000 42.857143 26.666666 Christopher c . miller 40.000000 35.294117 50.000000 33.333332 22.222221 40.000000 33.333332 27.272728 50.000000 100.000000 42.857143 100.000000 30.000000 Nancy Pelosi 0.000000 23.529411 40.000000 35.294117 25.000000 33.333332 25.000000 23.529411 26.666666 28.571428 26.666666 30.000000 100.000000 The different string representations of e.g. Donald Trump and Joe Biden have high mutual fuzzy ratios. In comparision representations of different persons have relatively small fuzzy ratios. You can think of this matrix as a correlation matrix, that shows the correlation between strings. Step 2: Forming Clusters Clusters of entities can be formed using the output from (1) using a naive approach clustering two string entities together, if their mutual fuzzy ratio exceeds a certain threshold. Computing the pairwise fuzzy ratios and forming the clusters can be done in one take by simply invoking the fuzzy_cluster function. clusters = fuzzy_cluster(PERSONS_NER, scorer=partial_token_set_ratio, cutoff=70, merge_output=True) pd.DataFrame.from_records(clusters) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement cluster_id 0 Donald Trump PER 0.778987 47 35 body Donald Trump 1 Donald Trump PER 0.466251 55 40 lead Donald Trump 2 J. biden PER 0.651698 77 21 body joe biden 3 joe biden PER 0.808048 31 72 body joe biden 4 Biden PER 0.899257 84 37 title joe biden 5 Bide PER 0.663719 93 61 body joe biden 6 mark esper PER 0.928882 32 68 title mark esper 7 Christopher c . miller PER 0.990222 94 6 title Christopher c . miller 8 jim mattis PER 0.747955 11 31 title jim mattis 9 Nancy Pelosi PER 0.201458 31 92 body Nancy Pelosi 10 trumps PER 0.977139 75 43 body Donald Trump 11 Trump PER 0.526853 81 38 lead Donald Trump 12 Donald PER 0.480917 34 32 body Donald Trump 13 miller PER 0.131910 59 57 body Christopher c . miller Note, that the original entities are now equipped with a 'cluster_id', assigning each of the entities to an entity cluster. We see from the results, that different string representations of e.g. 'Donald Trump' have been clustered together. As you see, the 'cluster_id' of each cluster is the longest string within the entity cluster. In this case we applied a partial_token_set_ratio and a cutoff threshold value of 75 on the pairwise fuzzy ratios. Depending on your use case, you should choose an appropriate scorer from rapidfuzz.fuzz and 'fine-tune' the cutoff threshold value on your own data. Step 3: Compute Prominence of Entity Clusters A na\u00efve approach for computing the 'prominence' of the different string clusters is to just count the number of nodes/strings in each cluster. This is the default behaviour of compute_prominence() . clusters = compute_prominence(clusters, merge_output=True) pd.DataFrame.from_records(clusters).sort_values('prominence_rank', ascending=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement cluster_id prominence_score prominence_rank 0 Donald Trump PER 0.778987 47 35 body Donald Trump 5.0 1 1 Donald Trump PER 0.466251 55 40 lead Donald Trump 5.0 1 10 trumps PER 0.977139 75 43 body Donald Trump 5.0 1 11 Trump PER 0.526853 81 38 lead Donald Trump 5.0 1 12 Donald PER 0.480917 34 32 body Donald Trump 5.0 1 2 J. biden PER 0.651698 77 21 body joe biden 4.0 2 3 joe biden PER 0.808048 31 72 body joe biden 4.0 2 4 Biden PER 0.899257 84 37 title joe biden 4.0 2 5 Bide PER 0.663719 93 61 body joe biden 4.0 2 7 Christopher c . miller PER 0.990222 94 6 title Christopher c . miller 2.0 3 13 miller PER 0.131910 59 57 body Christopher c . miller 2.0 3 6 mark esper PER 0.928882 32 68 title mark esper 1.0 4 8 jim mattis PER 0.747955 11 31 title jim mattis 1.0 4 9 Nancy Pelosi PER 0.201458 31 92 body Nancy Pelosi 1.0 4 In this case, the 'prominence score' of the 'Donald Trump' entity cluster is 5, because Donald Trump is mentioned 5 times in different variations. This is the highest frequency among the clusters and therefore the 'Donald Trump' cluster is scored as the most prominent cluster. The clusters are ranked by their prominence scores in the 'prominence rank' column. Step 4: Matching with Whitelists It can be useful to have one or more whitelists with specific entities of interest, when analyzing the output from NER. Assume, that we are only interested in Donald Trump and Joe Biden. We construct a minimal whitelist. whitelist = ['Donald Trump', 'Joe Biden'] Now, we can match it with our predicted entities using function match_whitelist . match_whitelist(clusters, whitelist, scorer=partial_token_set_ratio, score_cutoff=80, aggregate_cluster=True, to_dataframe=True).sort_values('prominence_rank', ascending=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement cluster_id prominence_score prominence_rank matches 0 Donald Trump PER 0.778987 47 35 body Donald Trump 5.0 1 [Donald Trump] 1 Donald Trump PER 0.466251 55 40 lead Donald Trump 5.0 1 [Donald Trump] 10 trumps PER 0.977139 75 43 body Donald Trump 5.0 1 [Donald Trump] 11 Trump PER 0.526853 81 38 lead Donald Trump 5.0 1 [Donald Trump] 12 Donald PER 0.480917 34 32 body Donald Trump 5.0 1 [Donald Trump] 2 J. biden PER 0.651698 77 21 body joe biden 4.0 2 [Joe Biden] 3 joe biden PER 0.808048 31 72 body joe biden 4.0 2 [Joe Biden] 4 Biden PER 0.899257 84 37 title joe biden 4.0 2 [Joe Biden] 5 Bide PER 0.663719 93 61 body joe biden 4.0 2 [Joe Biden] Whitelist matching can also be conducted using Whitelist subclasses. In the example below, NER output is compared to a Whitelist consisting of Cities . from fuzzup.whitelists import Cities LOCATIONS = [{'word': 'Viborg', 'entity_group': 'LOC', 'cluster_id' : 'Viborg'}, {'word': 'Uldum', 'entity_group': 'ORG', 'cluster_id' : 'Uldum' }] # initialize whitelist cities = Cities() # clustering and whitelist matching clusters = fuzzy_cluster(LOCATIONS) matches = cities(clusters, score_cutoff=90) matches INFO:fuzzup.whitelists:Loading whitelist: city INFO:fuzzup.whitelists:Done loading. [{'word': 'Viborg', 'entity_group': 'LOC', 'cluster_id': 'Viborg', 'matches': ['Visborg', 'Viborg'], 'mappings': [{'municipality': 'Mariagerfjord Kommune', 'eblocal_id': 10887, 'dawa_id': '12337669-ca46-6b98-e053-d480220a5a3f', 'lon_lat': (10.15028458, 56.73531236)}, {'municipality': 'Silkeborg Kommune', 'eblocal_id': 10790, 'dawa_id': nan, 'lon_lat': (9.36897659, 56.41681148)}]}]","title":"Showcase"},{"location":"showcase/#fuzzup-showcase","text":"fuzzup offers (1) a simple approach for clustering string entitities based on Levenshtein Distance using Fuzzy Matching in conjunction with a simple rule-based clustering method. fuzzup also provides (2) functions for computing the prominence of entity clusters resulting from (1). In this section we will go through the nuts and bolts of fuzzup by applying it to a realistic setting.","title":"fuzzup Showcase"},{"location":"showcase/#designed-for-handling-output-from-ner","text":"An important use-case for fuzzup is organizing, structuring and analyzing output from Named-Entity Recognition (=NER). For this reason fuzzup has been handtailored to fit the output from NER predictions from the Hugging Face transformers NER pipeline specifically.","title":"Designed for Handling Output from NER"},{"location":"showcase/#use-case","text":"First of, import dependencies needed later. from rapidfuzz.fuzz import partial_token_set_ratio import pandas as pd import numpy as np from fuzzup.datasets import simulate_ner_data from fuzzup.fuzz import ( fuzzy_cluster, compute_prominence, compute_fuzzy_matrix, ) from fuzzup.whitelists import match_whitelist Say, we have used a transformers Hugging Face NER pipeline to identify names of persons in a news article. The output from the algorithm is a list of string entities and looks like this (simulated data). PERSONS_NER = simulate_ner_data() pd.DataFrame.from_records(PERSONS_NER) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement 0 Donald Trump PER 0.778987 47 35 body 1 Donald Trump PER 0.466251 55 40 lead 2 J. biden PER 0.651698 77 21 body 3 joe biden PER 0.808048 31 72 body 4 Biden PER 0.899257 84 37 title 5 Bide PER 0.663719 93 61 body 6 mark esper PER 0.928882 32 68 title 7 Christopher c . miller PER 0.990222 94 6 title 8 jim mattis PER 0.747955 11 31 title 9 Nancy Pelosi PER 0.201458 31 92 body 10 trumps PER 0.977139 75 43 body 11 Trump PER 0.526853 81 38 lead 12 Donald PER 0.480917 34 32 body 13 miller PER 0.131910 59 57 body As you can see, the output is rather messy (partly due to the stochastic nature of the algorithm). Another reason for the output looking messy is, that for instance 'Joe Biden' has been mentioned a lot of times but in different ways, e.g. 'Joe Biden', 'J. Biden' and 'Biden'. We want to organize these strings entities by forming meaningful clusters from them, in which the entities are closely related based on their pairwise edit distances.","title":"Use-case"},{"location":"showcase/#workflow","text":"fuzzup offers functionality for: Computing all of the mutual string distances (Levensteihn Distances/fuzzy ratios) between the string entities Forming clusters of string entities based on the distances from (1) Computing prominence of the clusters from (2) based on the number of entity occurrences, their positions in the text etc. Matching entities (clusters) with entity whitelists Together these steps constitute an end-to-end approach for organizing and structuring the output from NER. Below we go through a simple example of the fuzzup workflow.","title":"Workflow"},{"location":"showcase/#step-1-compute-pairwise-edit-distances","text":"First, fuzzup computes pairwise fuzzy ratios for all pairs of string entities. Fuzzy ratios are numbers between 0 and 100 are measures of similarity between strings. They are derived from the Levenshtein distance - a string metric, that measures the distance between two strings. In short the Levenshtein distance (also known as 'edit distance') between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. fuzzup has a separate function compute_fuzzy_matrix for this, that presents the output - the mutual fuzzy ratios - as a cross-tabular matrix with all ratios. from fuzzup.fuzz import fuzzy_cluster persons = [x.get('word') for x in PERSONS_NER] compute_fuzzy_matrix(persons, scorer=partial_token_set_ratio) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Trump jim mattis Bide joe biden Donald Biden trumps Donald Trump mark esper miller J. biden Christopher c . miller Nancy Pelosi Trump 100.000000 25.000000 0.000000 0.000000 0.000000 0.000000 80.000000 100.000000 40.000000 33.333332 0.000000 40.000000 0.000000 jim mattis 25.000000 100.000000 33.333332 30.769230 18.181818 28.571428 44.444443 25.000000 40.000000 33.333332 26.666666 35.294117 23.529411 Bide 0.000000 33.333332 100.000000 75.000000 40.000000 100.000000 0.000000 25.000000 40.000000 50.000000 75.000000 50.000000 40.000000 joe biden 0.000000 30.769230 75.000000 100.000000 25.000000 80.000000 0.000000 25.000000 30.769230 40.000000 100.000000 33.333332 35.294117 Donald 0.000000 18.181818 40.000000 25.000000 100.000000 33.333332 0.000000 100.000000 22.222221 22.222221 28.571428 22.222221 25.000000 Biden 0.000000 28.571428 100.000000 80.000000 33.333332 100.000000 0.000000 25.000000 33.333332 40.000000 88.888885 40.000000 33.333332 trumps 80.000000 44.444443 0.000000 0.000000 0.000000 0.000000 100.000000 80.000000 33.333332 28.571428 0.000000 33.333332 25.000000 Donald Trump 100.000000 25.000000 25.000000 25.000000 100.000000 25.000000 80.000000 100.000000 28.571428 33.333332 18.181818 27.272728 22.222221 mark esper 40.000000 37.500000 40.000000 30.769230 22.222221 33.333332 33.333332 28.571428 100.000000 40.000000 22.222221 50.000000 26.666666 miller 33.333332 33.333332 50.000000 40.000000 25.000000 40.000000 25.000000 33.333332 40.000000 100.000000 40.000000 100.000000 28.571428 J. biden 0.000000 26.666666 75.000000 100.000000 28.571428 88.888885 0.000000 18.181818 22.222221 40.000000 100.000000 42.857143 26.666666 Christopher c . miller 40.000000 35.294117 50.000000 33.333332 22.222221 40.000000 33.333332 27.272728 50.000000 100.000000 42.857143 100.000000 30.000000 Nancy Pelosi 0.000000 23.529411 40.000000 35.294117 25.000000 33.333332 25.000000 23.529411 26.666666 28.571428 26.666666 30.000000 100.000000 The different string representations of e.g. Donald Trump and Joe Biden have high mutual fuzzy ratios. In comparision representations of different persons have relatively small fuzzy ratios. You can think of this matrix as a correlation matrix, that shows the correlation between strings.","title":"Step 1: Compute Pairwise Edit Distances"},{"location":"showcase/#step-2-forming-clusters","text":"Clusters of entities can be formed using the output from (1) using a naive approach clustering two string entities together, if their mutual fuzzy ratio exceeds a certain threshold. Computing the pairwise fuzzy ratios and forming the clusters can be done in one take by simply invoking the fuzzy_cluster function. clusters = fuzzy_cluster(PERSONS_NER, scorer=partial_token_set_ratio, cutoff=70, merge_output=True) pd.DataFrame.from_records(clusters) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement cluster_id 0 Donald Trump PER 0.778987 47 35 body Donald Trump 1 Donald Trump PER 0.466251 55 40 lead Donald Trump 2 J. biden PER 0.651698 77 21 body joe biden 3 joe biden PER 0.808048 31 72 body joe biden 4 Biden PER 0.899257 84 37 title joe biden 5 Bide PER 0.663719 93 61 body joe biden 6 mark esper PER 0.928882 32 68 title mark esper 7 Christopher c . miller PER 0.990222 94 6 title Christopher c . miller 8 jim mattis PER 0.747955 11 31 title jim mattis 9 Nancy Pelosi PER 0.201458 31 92 body Nancy Pelosi 10 trumps PER 0.977139 75 43 body Donald Trump 11 Trump PER 0.526853 81 38 lead Donald Trump 12 Donald PER 0.480917 34 32 body Donald Trump 13 miller PER 0.131910 59 57 body Christopher c . miller Note, that the original entities are now equipped with a 'cluster_id', assigning each of the entities to an entity cluster. We see from the results, that different string representations of e.g. 'Donald Trump' have been clustered together. As you see, the 'cluster_id' of each cluster is the longest string within the entity cluster. In this case we applied a partial_token_set_ratio and a cutoff threshold value of 75 on the pairwise fuzzy ratios. Depending on your use case, you should choose an appropriate scorer from rapidfuzz.fuzz and 'fine-tune' the cutoff threshold value on your own data.","title":"Step 2: Forming Clusters"},{"location":"showcase/#step-3-compute-prominence-of-entity-clusters","text":"A na\u00efve approach for computing the 'prominence' of the different string clusters is to just count the number of nodes/strings in each cluster. This is the default behaviour of compute_prominence() . clusters = compute_prominence(clusters, merge_output=True) pd.DataFrame.from_records(clusters).sort_values('prominence_rank', ascending=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement cluster_id prominence_score prominence_rank 0 Donald Trump PER 0.778987 47 35 body Donald Trump 5.0 1 1 Donald Trump PER 0.466251 55 40 lead Donald Trump 5.0 1 10 trumps PER 0.977139 75 43 body Donald Trump 5.0 1 11 Trump PER 0.526853 81 38 lead Donald Trump 5.0 1 12 Donald PER 0.480917 34 32 body Donald Trump 5.0 1 2 J. biden PER 0.651698 77 21 body joe biden 4.0 2 3 joe biden PER 0.808048 31 72 body joe biden 4.0 2 4 Biden PER 0.899257 84 37 title joe biden 4.0 2 5 Bide PER 0.663719 93 61 body joe biden 4.0 2 7 Christopher c . miller PER 0.990222 94 6 title Christopher c . miller 2.0 3 13 miller PER 0.131910 59 57 body Christopher c . miller 2.0 3 6 mark esper PER 0.928882 32 68 title mark esper 1.0 4 8 jim mattis PER 0.747955 11 31 title jim mattis 1.0 4 9 Nancy Pelosi PER 0.201458 31 92 body Nancy Pelosi 1.0 4 In this case, the 'prominence score' of the 'Donald Trump' entity cluster is 5, because Donald Trump is mentioned 5 times in different variations. This is the highest frequency among the clusters and therefore the 'Donald Trump' cluster is scored as the most prominent cluster. The clusters are ranked by their prominence scores in the 'prominence rank' column.","title":"Step 3: Compute Prominence of Entity Clusters"},{"location":"showcase/#step-4-matching-with-whitelists","text":"It can be useful to have one or more whitelists with specific entities of interest, when analyzing the output from NER. Assume, that we are only interested in Donald Trump and Joe Biden. We construct a minimal whitelist. whitelist = ['Donald Trump', 'Joe Biden'] Now, we can match it with our predicted entities using function match_whitelist . match_whitelist(clusters, whitelist, scorer=partial_token_set_ratio, score_cutoff=80, aggregate_cluster=True, to_dataframe=True).sort_values('prominence_rank', ascending=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word entity_group score start end placement cluster_id prominence_score prominence_rank matches 0 Donald Trump PER 0.778987 47 35 body Donald Trump 5.0 1 [Donald Trump] 1 Donald Trump PER 0.466251 55 40 lead Donald Trump 5.0 1 [Donald Trump] 10 trumps PER 0.977139 75 43 body Donald Trump 5.0 1 [Donald Trump] 11 Trump PER 0.526853 81 38 lead Donald Trump 5.0 1 [Donald Trump] 12 Donald PER 0.480917 34 32 body Donald Trump 5.0 1 [Donald Trump] 2 J. biden PER 0.651698 77 21 body joe biden 4.0 2 [Joe Biden] 3 joe biden PER 0.808048 31 72 body joe biden 4.0 2 [Joe Biden] 4 Biden PER 0.899257 84 37 title joe biden 4.0 2 [Joe Biden] 5 Bide PER 0.663719 93 61 body joe biden 4.0 2 [Joe Biden] Whitelist matching can also be conducted using Whitelist subclasses. In the example below, NER output is compared to a Whitelist consisting of Cities . from fuzzup.whitelists import Cities LOCATIONS = [{'word': 'Viborg', 'entity_group': 'LOC', 'cluster_id' : 'Viborg'}, {'word': 'Uldum', 'entity_group': 'ORG', 'cluster_id' : 'Uldum' }] # initialize whitelist cities = Cities() # clustering and whitelist matching clusters = fuzzy_cluster(LOCATIONS) matches = cities(clusters, score_cutoff=90) matches INFO:fuzzup.whitelists:Loading whitelist: city INFO:fuzzup.whitelists:Done loading. [{'word': 'Viborg', 'entity_group': 'LOC', 'cluster_id': 'Viborg', 'matches': ['Visborg', 'Viborg'], 'mappings': [{'municipality': 'Mariagerfjord Kommune', 'eblocal_id': 10887, 'dawa_id': '12337669-ca46-6b98-e053-d480220a5a3f', 'lon_lat': (10.15028458, 56.73531236)}, {'municipality': 'Silkeborg Kommune', 'eblocal_id': 10790, 'dawa_id': nan, 'lon_lat': (9.36897659, 56.41681148)}]}]","title":"Step 4: Matching with Whitelists"}]}